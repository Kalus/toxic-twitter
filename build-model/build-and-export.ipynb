{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv, codecs\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import keras.backend\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import unidecode\n",
    "\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Conv1D, MaxPooling1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(device_count={'GPU': 1, 'CPU': 4}) \n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json_file = 'ascii-3-model/model-ascii.json'\n",
    "model_h5_file = 'ascii-3-model/model-ascii.h5'\n",
    "char_to_index = 'ascii-3-model/ascii-char-map.json'\n",
    "\n",
    "max_features = 200\n",
    "maxlen = 500\n",
    "embedding_size = 12\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have `train.csv` downloaded from\n",
    "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write out tokenizer index\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 500, 12)           864       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 500, 100)          4900      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 125, 100)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 125, 120)          57960     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                6050      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 70,080\n",
      "Trainable params: 70,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train, \n",
    "    train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]], \n",
    "    test_size=0.10, random_state=42\n",
    ")\n",
    "\n",
    "list_sentences_train = X_train['comment_text'].apply(unidecode.unidecode)\n",
    "list_sentences_test = X_test['comment_text'].apply(unidecode.unidecode)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features,char_level=True)\n",
    "\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "\n",
    "\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_sentences_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "\n",
    "with open(char_to_index, 'w') as f:\n",
    "    f.write(json.dumps(tokenizer.word_index))\n",
    "    print('write out tokenizer index')\n",
    "\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_sentences_test, maxlen=maxlen)\n",
    "\n",
    "def get_model(embed_size,dropout=0.2):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(len(tokenizer.word_index)+1, embed_size)(inp)\n",
    "    x = Conv1D(filters=100, kernel_size=4, padding='same', activation='relu')(x)\n",
    "    x = MaxPooling1D(pool_size=4)(x)\n",
    "\n",
    "    x = Bidirectional(GRU(60, return_sequences=True,\n",
    "                          name='lstm_layer', dropout=dropout,\n",
    "                          recurrent_dropout=0.2))(x)\n",
    "\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "\n",
    "    x = Dense(50, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                   metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model(embed_size=embedding_size, dropout=dropout)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      "143613/143613 [==============================] - 647s 5ms/step - loss: 0.0921 - acc: 0.9732 - val_loss: 0.0642 - val_acc: 0.9793\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 6\n",
    "hist = model.fit(X_t, y_train, \n",
    "                 batch_size=batch_size,\n",
    "                 epochs=epochs,\n",
    "                 validation_data=(X_te, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(model_json_file, \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(model_h5_file)\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
