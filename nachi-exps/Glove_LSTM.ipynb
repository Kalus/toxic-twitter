{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, GRU, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Conv1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "from keras.models import model_from_json\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import keras.backend\n",
    "import unidecode\n",
    "import json\n",
    "import regex as re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "EMBEDSIZE = 50\n",
    "MAXFEATURES = 2000\n",
    "MAXLEN = 200\n",
    "batch_size = 64\n",
    "epochs = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153164, 7)\n",
      "(153164, 2)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test  = pd.read_csv('data/test.csv')\n",
    "test_labels = pd.read_csv('data/test_labels.csv')\n",
    "EMBEDDING_FILE = f'glove-twitter-27B/glove.twitter.27B.50d.txt'\n",
    "\n",
    "print(test_labels.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63978, 7)\n",
      "(63978, 2)\n"
     ]
    }
   ],
   "source": [
    "idx = test_labels.index[test_labels['toxic'] == -1].tolist()\n",
    "np.array(idx).shape\n",
    "test_labels = test_labels.drop(test_labels.index[idx])\n",
    "test = test.drop(test.index[idx])\n",
    "print(test_labels.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def glove_preprocess(text):\n",
    "    \"\"\"\n",
    "    adapted from https://nlp.stanford.edu/projects/glove/preprocess-twitter.rb\n",
    "\n",
    "    \"\"\"\n",
    "    # Different regex parts for smiley faces\n",
    "    eyes = \"[8:=;]\"\n",
    "    nose = \"['`\\-]?\"\n",
    "    text = re.sub(\"https?:* \", \"<URL>\", text)\n",
    "    text = re.sub(\"www.* \", \"<URL>\", text)\n",
    "    text = re.sub(\"\\[\\[User(.*)\\|\", '<USER>', text)\n",
    "    text = re.sub(\"<3\", '<HEART>', text)\n",
    "    text = re.sub(\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<NUMBER>\", text)\n",
    "    text = re.sub(eyes + nose + \"[Dd)]\", '<SMILE>', text)\n",
    "    text = re.sub(\"[(d]\" + nose + eyes, '<SMILE>', text)\n",
    "    text = re.sub(eyes + nose + \"p\", '<LOLFACE>', text)\n",
    "    text = re.sub(eyes + nose + \"\\(\", '<SADFACE>', text)\n",
    "    text = re.sub(\"\\)\" + nose + eyes, '<SADFACE>', text)\n",
    "    text = re.sub(eyes + nose + \"[/|l*]\", '<NEUTRALFACE>', text)\n",
    "    text = re.sub(\"/\", \" / \", text)\n",
    "    text = re.sub(\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<NUMBER>\", text)\n",
    "    text = re.sub(\"([!]){2,}\", \"! <REPEAT>\", text)\n",
    "    text = re.sub(\"([?]){2,}\", \"? <REPEAT>\", text)\n",
    "    text = re.sub(\"([.]){2,}\", \". <REPEAT>\", text)\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    text = pattern.sub(r\"\\1\" + \" <ELONG>\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train,\n",
    "                                        train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]],\n",
    "                                        test_size = 0.10, random_state = 42)\n",
    "list_sentences_train = X_train[\"comment_text\"].apply(glove_preprocess)\n",
    "list_sentences_test = X_test[\"comment_text\"].apply(glove_preprocess)\n",
    "list_sentences_final_test = test[\"comment_text\"].apply(glove_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAXFEATURES,char_level=True)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_sentences_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "list_sentences_final_test = tokenizer.texts_to_sequences(list_sentences_final_test)\n",
    "\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=MAXLEN)\n",
    "X_te = pad_sequences(list_sentences_test, maxlen=MAXLEN)\n",
    "X_test = pad_sequences(list_sentences_final_test, maxlen = MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model(embedding_matrix, dropout = 0.2):\n",
    "    inp = Input(shape=(MAXLEN,))\n",
    "    x = Embedding(MAXFEATURES, EMBEDSIZE, weights=[ embedding_matrix])(inp)\n",
    "    x = Conv1D(filters = 100, kernel_size = 4, padding = 'same', activation = 'relu' )(x)\n",
    "    x = MaxPooling1D(pool_size =4)(x)\n",
    "    x = Bidirectional(GRU(60, return_sequences=True, dropout=dropout, recurrent_dropout=0.2))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dense(50, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(6, activation = \"sigmoid\")(x)\n",
    "    model = Model(inputs= inp, outputs = x)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_LSTM_model(embedding_matrix, dropout = 0.2):\n",
    "    inp = Input(shape=(MAXLEN,))\n",
    "    x = Embedding(MAXFEATURES, EMBEDSIZE, weights=[ embedding_matrix])(inp)\n",
    "    x = Conv1D(filters = 100, kernel_size = 4, padding = 'same', activation = 'relu' )(x)\n",
    "    x = MaxPooling1D(pool_size =4)(x)\n",
    "    x = Bidirectional(GRU(60, return_sequences=True, dropout=dropout, recurrent_dropout=0.2))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dense(50, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(6, activation = \"sigmoid\")(x)\n",
    "    model = Model(inputs= inp, outputs = x)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE,encoding=\"utf8\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "num_words = min(MAXFEATURES, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDSIZE))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAXFEATURES:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 200, 50)           100000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 200, 100)          20100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 50, 120)           57960     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                6050      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 184,416\n",
      "Trainable params: 184,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model(embedding_matrix, dropout=0.2)\n",
    "model.summary()\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 200, 50)           100000    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 200, 100)          20100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 50, 120)           57960     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                6050      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 184,416\n",
      "Trainable params: 184,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model = get_LSTM_model(embedding_matrix, dropout=0.2)\n",
    "lstm_model.summary()\n",
    "plot_model(lstm_model, to_file='model_lstm_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Call backs\n",
    "wtFile = \"weights.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(wtFile, monitor = 'val_loss', verbose=1, save_best_only=True, mode = 'min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "callbacks_list = [checkpoint, early] #early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/3\n",
      "143613/143613 [==============================] - 323s 2ms/step - loss: 0.0989 - acc: 0.9718 - val_loss: 0.0707 - val_acc: 0.9782\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.07072, saving model to weights.best.hdf5\n",
      "Epoch 2/3\n",
      "143613/143613 [==============================] - 325s 2ms/step - loss: 0.0692 - acc: 0.9786 - val_loss: 0.0614 - val_acc: 0.9804\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.07072 to 0.06144, saving model to weights.best.hdf5\n",
      "Epoch 3/3\n",
      "143613/143613 [==============================] - 317s 2ms/step - loss: 0.0630 - acc: 0.9799 - val_loss: 0.0612 - val_acc: 0.9805\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.06144 to 0.06117, saving model to weights.best.hdf5\n",
      "Saved Model Weights to file!!\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_t, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_te,y_test), callbacks=callbacks_list)\n",
    "print(\"Saved Model Weights to file!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(wtFile)\n",
    "\n",
    "# y_test = model.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_model.load_weights('weights.best_lstm.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model to file!!\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "print(\"Saved Model to file!!\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Tokenizer to file!!\n"
     ]
    }
   ],
   "source": [
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(\"Saved Tokenizer to file!!\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Embedding Index to file!!!\n"
     ]
    }
   ],
   "source": [
    "with open('embedding_index.pickle', 'wb') as handle:\n",
    "    pickle.dump(embeddings_index, handle, protocol= pickle.HIGHEST_PROTOCOL)\n",
    "print(\"Saved Embedding Index to file!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/3\n",
      "143613/143613 [==============================] - 242s 2ms/step - loss: 0.1018 - acc: 0.9714 - val_loss: 0.0767 - val_acc: 0.9772\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.07673, saving model to weights.best_lstm.hdf5\n",
      "Epoch 2/3\n",
      "143613/143613 [==============================] - 235s 2ms/step - loss: 0.0699 - acc: 0.9784 - val_loss: 0.0645 - val_acc: 0.9791\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.07673 to 0.06452, saving model to weights.best_lstm.hdf5\n",
      "Epoch 3/3\n",
      "143613/143613 [==============================] - 229s 2ms/step - loss: 0.0637 - acc: 0.9796 - val_loss: 0.0580 - val_acc: 0.9811\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.06452 to 0.05795, saving model to weights.best_lstm.hdf5\n",
      "Saved LSTM Model to file!!\n",
      "Saved  LSTM Model Weights to file!!\n"
     ]
    }
   ],
   "source": [
    "wtFile = \"weights.best_lstm.hdf5\"\n",
    "checkpoint = ModelCheckpoint(wtFile, monitor = 'val_loss', verbose=1, save_best_only=True, mode = 'min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "callbacks_list = [checkpoint, early] #early\n",
    "lstm_model.fit(X_t, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_te,y_test),callbacks=callbacks_list)\n",
    "model_lstm_json = lstm_model.to_json()\n",
    "with open(\"model_lstm.json\", \"w\") as json_file:\n",
    "    json_file.write(model_lstm_json)\n",
    "print(\"Saved LSTM Model to file!!\") \n",
    "print(\"Saved  LSTM Model Weights to file!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 100, 50)           100000    \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 100, 100)          20100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 25, 100)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 25, 120)           57960     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50)                6050      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 184,416\n",
      "Trainable params: 184,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'binary_crossentropy'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.summary()\n",
    "lstm_model.metrics\n",
    "lstm_model.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_OF_WORDS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_sentence(toxic_text, words_to_highlight):\n",
    "    listOfWords = toxic_text.split()\n",
    "    out_str = ''\n",
    "    for i in range(len(listOfWords)):\n",
    "        if i in words_to_highlight:\n",
    "            for k in range(NUM_OF_WORDS):\n",
    "                listOfWords[i+k] = bcolors.WARNING + listOfWords[i+k] + bcolors.ENDC\n",
    "#             listOfWords[i+1] = bcolors.WARNING + listOfWords[i+1] + bcolors.ENDC\n",
    "    out_str = ' '.join(listOfWords)\n",
    "    highlight_txt = bcolors.FAIL + 'Highlighted' + bcolors.ENDC\n",
    "    print(highlight_txt, ' - ', out_str)\n",
    "#     print(out_str)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_toxicity(toxic_text, word):\n",
    "#     print(toxic_text)\n",
    "    text_pp = glove_preprocess(toxic_text)\n",
    "#     print(text_pp)\n",
    "    tok_text = tokenizer.texts_to_sequences([text_pp])\n",
    "#     print(tok_text)\n",
    "    x = pad_sequences(tok_text, maxlen=MAXLEN)\n",
    "#     print(x.shape)\n",
    "    y = model.predict(x)\n",
    "#     print(word, '-', np.round(y,3))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_representation(toxic_text):\n",
    "    listOfWords = toxic_text.split()\n",
    "#     print(listOfWords)\n",
    "    nWords = len(listOfWords)\n",
    "    baseline = predict_toxicity(toxic_text, 'BASELINE')\n",
    "    words_to_highlight = []\n",
    "    for i in range(nWords-(NUM_OF_WORDS-1)):\n",
    "        listOfWords = toxic_text.split()\n",
    "        curWord = ' '.join(listOfWords[i:i+2])\n",
    "    #     print('Iter - ', i)\n",
    "    #     print(listOfWords)\n",
    "        for k in range(NUM_OF_WORDS):\n",
    "            del listOfWords[i]\n",
    "    #     del listOfWords[i]\n",
    "        cur_toxic_text = ' '.join(listOfWords)\n",
    "        curValue = predict_toxicity(cur_toxic_text, curWord)\n",
    "        if curValue[0][0] < baseline[0][0]:\n",
    "            words_to_highlight.append(i)\n",
    "#     print(words_to_highlight)        \n",
    "    highlight_sentence(toxic_text, words_to_highlight)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Toxic Comments -  6090\n",
      "\u001b[1mOriginal   \u001b[0m  -  == Arabs are committing genocide in Iraq, but no protests in Europe. == \n",
      "\n",
      " May Europe also burn in hell.\n",
      "\u001b[91mHighlighted\u001b[0m  -  \u001b[93m==\u001b[0m \u001b[93mArabs\u001b[0m \u001b[93mare\u001b[0m committing \u001b[93mgenocide\u001b[0m \u001b[93min\u001b[0m Iraq, \u001b[93mbut\u001b[0m \u001b[93mno\u001b[0m protests in Europe. == \u001b[93mMay\u001b[0m \u001b[93mEurope\u001b[0m \u001b[93malso\u001b[0m \u001b[93mburn\u001b[0m in \u001b[93mhell.\u001b[0m\n",
      "\u001b[1mOriginal   \u001b[0m  -  DJ Robinson is gay as hell! he sucks his dick so much!!!!!\n",
      "\u001b[91mHighlighted\u001b[0m  -  DJ Robinson is \u001b[93mgay\u001b[0m \u001b[93mas\u001b[0m \u001b[93mhell!\u001b[0m \u001b[93mhe\u001b[0m \u001b[93msucks\u001b[0m his \u001b[93mdick\u001b[0m \u001b[93mso\u001b[0m \u001b[93mmuch!!!!!\u001b[0m\n",
      "\u001b[1mOriginal   \u001b[0m  -  :Fuck off, you anti-semitic cunt.  |\n",
      "\u001b[91mHighlighted\u001b[0m  -  \u001b[93m:Fuck\u001b[0m \u001b[93moff,\u001b[0m \u001b[93myou\u001b[0m anti-semitic \u001b[93mcunt.\u001b[0m \u001b[93m|\u001b[0m\n",
      "\u001b[1mOriginal   \u001b[0m  -  How dare you vandalize that page about the HMS Beagle! Don't vandalize again, demon!\n",
      "\u001b[91mHighlighted\u001b[0m  -  How dare \u001b[93myou\u001b[0m vandalize that page \u001b[93mabout\u001b[0m \u001b[93mthe\u001b[0m \u001b[93mHMS\u001b[0m \u001b[93mBeagle!\u001b[0m \u001b[93mDon't\u001b[0m \u001b[93mvandalize\u001b[0m \u001b[93magain,\u001b[0m \u001b[93mdemon!\u001b[0m\n",
      "\u001b[1mOriginal   \u001b[0m  -  ::No, he is an arrogant, self serving, immature idiot. Get it right.\n",
      "\u001b[91mHighlighted\u001b[0m  -  ::No, \u001b[93mhe\u001b[0m \u001b[93mis\u001b[0m \u001b[93man\u001b[0m arrogant, \u001b[93mself\u001b[0m serving, immature \u001b[93midiot.\u001b[0m Get it right.\n",
      "\u001b[1mOriginal   \u001b[0m  -  :Eek, but shes cute in an earthy kind of way. Cant sing for shit though. Thanks for giving me an unhappy memory.\n",
      "\u001b[91mHighlighted\u001b[0m  -  :Eek, but \u001b[93mshes\u001b[0m cute \u001b[93min\u001b[0m \u001b[93man\u001b[0m \u001b[93mearthy\u001b[0m \u001b[93mkind\u001b[0m \u001b[93mof\u001b[0m way. Cant \u001b[93msing\u001b[0m for \u001b[93mshit\u001b[0m \u001b[93mthough.\u001b[0m Thanks for giving me an \u001b[93munhappy\u001b[0m memory.\n",
      "\u001b[1mOriginal   \u001b[0m  -  ::::Well, it sucks to have a university to be nicknameless. And it's the first time in NCAA history that it has happened. /\n",
      "\u001b[91mHighlighted\u001b[0m  -  ::::Well, it \u001b[93msucks\u001b[0m \u001b[93mto\u001b[0m have \u001b[93ma\u001b[0m \u001b[93muniversity\u001b[0m \u001b[93mto\u001b[0m \u001b[93mbe\u001b[0m \u001b[93mnicknameless.\u001b[0m \u001b[93mAnd\u001b[0m it's the \u001b[93mfirst\u001b[0m time \u001b[93min\u001b[0m NCAA history that \u001b[93mit\u001b[0m has \u001b[93mhappened.\u001b[0m /\n",
      "\u001b[1mOriginal   \u001b[0m  -  == Argh == \n",
      "\n",
      " Some random idiot deleted the whole Japan article <.\n",
      "\u001b[91mHighlighted\u001b[0m  -  \u001b[93m==\u001b[0m \u001b[93mArgh\u001b[0m == Some \u001b[93mrandom\u001b[0m \u001b[93midiot\u001b[0m \u001b[93mdeleted\u001b[0m the \u001b[93mwhole\u001b[0m \u001b[93mJapan\u001b[0m article \u001b[93m<.\u001b[0m\n",
      "\u001b[1mOriginal   \u001b[0m  -  (we hate america and we are going to bomb the shit out of there cities, Queza;s will rain)\n",
      "\u001b[91mHighlighted\u001b[0m  -  (we hate america and we \u001b[93mare\u001b[0m \u001b[93mgoing\u001b[0m to \u001b[93mbomb\u001b[0m the \u001b[93mshit\u001b[0m out \u001b[93mof\u001b[0m there cities, Queza;s will rain)\n",
      "\u001b[1mOriginal   \u001b[0m  -  == Bold textYOU SUCK!!! == \n",
      "\n",
      "  \n",
      "\n",
      " U SUCK HANNAH MONTANA\n",
      "\u001b[91mHighlighted\u001b[0m  -  == Bold textYOU \u001b[93mSUCK!!!\u001b[0m == \u001b[93mU\u001b[0m \u001b[93mSUCK\u001b[0m \u001b[93mHANNAH\u001b[0m MONTANA\n"
     ]
    }
   ],
   "source": [
    "idx =test_labels.index[test_labels['toxic'] ==1].tolist()\n",
    "print('Number of Toxic Comments - ', len(idx))\n",
    "for i in range(10):\n",
    "    toxic_idx = idx[i]\n",
    "    toxic_text = test['comment_text'][toxic_idx]\n",
    "    orignial_txt = bcolors.BOLD + 'Original   ' + bcolors.ENDC\n",
    "    print(orignial_txt, ' - ', toxic_text)\n",
    "    text_representation(toxic_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63978/63978 [==============================] - 25s 385us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.39151233960310006, 0.0025321204163931349]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.evaluate(X_test, test_labels, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predicted = lstm_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63978/63978 [==============================] - 12s 192us/step\n"
     ]
    }
   ],
   "source": [
    "y_predicted = lstm_model.predict([X_test], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " ..., \n",
      " [0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0]]\n",
      "[ 0.43294343  0.33663076  0.470263    0.36656839  0.44876203  0.39546281]\n"
     ]
    }
   ],
   "source": [
    "y_pred = (y_predicted == y_predicted.max(axis=1)[:,None]).astype(int)\n",
    "print(y_pred)\n",
    "print(y_predicted[-2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " ..., \n",
      " [0 0 0 0 0 0]\n",
      " [1 0 1 0 1 0]\n",
      " [0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "y_true = np.array(test_labels)\n",
    "y_true = y_true[:,1:]\n",
    "print(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63978/63978 [==============================] - 29s 450us/step\n"
     ]
    }
   ],
   "source": [
    "y_predicted = model.predict([X_test], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.24215028e-03   4.76808509e-06   5.76814346e-04   1.55739726e-05\n",
      "    7.61029485e-04   6.87712673e-05]\n",
      " [  1.65921282e-02   3.43151223e-05   2.45818496e-03   1.28129454e-04\n",
      "    3.00476723e-03   4.45229874e-04]\n",
      " [  1.06181921e-02   1.75847053e-05   1.67270552e-03   2.93064240e-05\n",
      "    1.86486216e-03   1.52371911e-04]\n",
      " ..., \n",
      " [  3.39214504e-01   1.34784477e-02   8.66590887e-02   3.15312371e-02\n",
      "    1.76694095e-01   4.85188439e-02]\n",
      " [  9.26178515e-01   7.36789703e-02   8.10041368e-01   2.19827201e-02\n",
      "    5.57457924e-01   6.67238161e-02]\n",
      " [  1.28179435e-02   1.73915632e-05   2.00568093e-03   4.35755865e-05\n",
      "    2.16767122e-03   2.06897355e-04]]\n"
     ]
    }
   ],
   "source": [
    "print(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fffac2a094c8e0e2' 1 0 1 0 1 0]\n",
      "[ 0.92617851  0.07367897  0.81004137  0.02198272  0.55745792  0.06672382]\n"
     ]
    }
   ],
   "source": [
    "print(y_true[-2,:])\n",
    "print(y_predicted[-2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63978/63978 [==============================] - 35s 544us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.075009704749482611, 0.97380088891535543]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_true, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63978/63978 [==============================] - 38s 594us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.073987366234456106, 0.97335542349150694]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.evaluate(X_test, y_true, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to load a weight file containing 5 layers into a model with 0 layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-2ea436e9d141>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwtFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Cyrus\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[0;32m   1164\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1165\u001b[0m                 saving.load_weights_from_hdf5_group(\n\u001b[1;32m-> 1166\u001b[1;33m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[0;32m   1167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Cyrus\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[1;34m(f, layers, reshape)\u001b[0m\n\u001b[0;32m   1028\u001b[0m                          \u001b[1;34m'containing '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1029\u001b[0m                          \u001b[1;34m' layers into a model with '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1030\u001b[1;33m                          str(len(filtered_layers)) + ' layers.')\n\u001b[0m\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m     \u001b[1;31m# We batch weight value assignments in a single backend call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: You are trying to load a weight file containing 5 layers into a model with 0 layers."
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "model.load_weights(wtFile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open('tokenizer.pickle','rb')\n",
    "new_dict = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.text.Tokenizer at 0x25feb66acc0>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
