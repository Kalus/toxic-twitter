{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, GRU, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Conv1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "from keras.models import model_from_json\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import keras.backend\n",
    "import unidecode\n",
    "import json\n",
    "import regex as re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "EMBEDSIZE = 50\n",
    "MAXFEATURES = 2000\n",
    "MAXLEN = 200\n",
    "batch_size = 64\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153164, 7)\n",
      "(153164, 2)\n",
      "(63978, 7)\n",
      "(63978, 2)\n"
     ]
    }
   ],
   "source": [
    "#Data\n",
    "test  = pd.read_csv('data/test.csv')\n",
    "test_labels = pd.read_csv('data/test_labels.csv')\n",
    "# EMBEDDING_FILE = f'glove-twitter-27B/glove.twitter.27B.50d.txt'\n",
    "\n",
    "print(test_labels.shape)\n",
    "print(test.shape)\n",
    "\n",
    "idx = test_labels.index[test_labels['toxic'] == -1].tolist()\n",
    "np.array(idx).shape\n",
    "test_labels = test_labels.drop(test_labels.index[idx])\n",
    "test = test.drop(test.index[idx])\n",
    "print(test_labels.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def glove_preprocess(text):\n",
    "    \"\"\"\n",
    "    adapted from https://nlp.stanford.edu/projects/glove/preprocess-twitter.rb\n",
    "\n",
    "    \"\"\"\n",
    "    # Different regex parts for smiley faces\n",
    "    eyes = \"[8:=;]\"\n",
    "    nose = \"['`\\-]?\"\n",
    "    text = re.sub(\"https?:* \", \"<URL>\", text)\n",
    "    text = re.sub(\"www.* \", \"<URL>\", text)\n",
    "    text = re.sub(\"\\[\\[User(.*)\\|\", '<USER>', text)\n",
    "    text = re.sub(\"<3\", '<HEART>', text)\n",
    "    text = re.sub(\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<NUMBER>\", text)\n",
    "    text = re.sub(eyes + nose + \"[Dd)]\", '<SMILE>', text)\n",
    "    text = re.sub(\"[(d]\" + nose + eyes, '<SMILE>', text)\n",
    "    text = re.sub(eyes + nose + \"p\", '<LOLFACE>', text)\n",
    "    text = re.sub(eyes + nose + \"\\(\", '<SADFACE>', text)\n",
    "    text = re.sub(\"\\)\" + nose + eyes, '<SADFACE>', text)\n",
    "    text = re.sub(eyes + nose + \"[/|l*]\", '<NEUTRALFACE>', text)\n",
    "    text = re.sub(\"/\", \" / \", text)\n",
    "    text = re.sub(\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<NUMBER>\", text)\n",
    "    text = re.sub(\"([!]){2,}\", \"! <REPEAT>\", text)\n",
    "    text = re.sub(\"([?]){2,}\", \"? <REPEAT>\", text)\n",
    "    text = re.sub(\"([.]){2,}\", \". <REPEAT>\", text)\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    text = pattern.sub(r\"\\1\" + \" <ELONG>\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model(embedding_matrix, dropout = 0.2):\n",
    "    inp = Input(shape=(MAXLEN,))\n",
    "    x = Embedding(MAXFEATURES, EMBEDSIZE, weights=[ embedding_matrix])(inp)\n",
    "    x = Conv1D(filters = 100, kernel_size = 4, padding = 'same', activation = 'relu' )(x)\n",
    "    x = MaxPooling1D(pool_size =4)(x)\n",
    "    x = Bidirectional(GRU(60, return_sequences=True, dropout=dropout, recurrent_dropout=0.2))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dense(50, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(6, activation = \"sigmoid\")(x)\n",
    "    model = Model(inputs= inp, outputs = x)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Tokenizer\n",
      "Loaded Embedding Index\n"
     ]
    }
   ],
   "source": [
    "tok_file = open('tokenizer.pickle','rb')\n",
    "tokenizer = pickle.load(tok_file)\n",
    "tok_file.close()\n",
    "print('Loaded Tokenizer')\n",
    "\n",
    "emb_idx_file = open('embedding_index.pickle','rb')\n",
    "embeddings_index = pickle.load(emb_idx_file)\n",
    "emb_idx_file.close()\n",
    "print('Loaded Embedding Index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "num_words = min(MAXFEATURES, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDSIZE))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAXFEATURES:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 200, 50)           100000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 200, 100)          20100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 50, 120)           57960     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                6050      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 184,416\n",
      "Trainable params: 184,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model(embedding_matrix, dropout=0.2)\n",
    "model.summary()\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "wtFile = \"weights.best.hdf5\"\n",
    "model.load_weights(wtFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_OF_WORDS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def highlight_sentence(toxic_text, words_to_highlight):\n",
    "    listOfWords = toxic_text.split()\n",
    "    out_str = ''\n",
    "    for i in range(len(listOfWords)):\n",
    "        if i in words_to_highlight:\n",
    "            for k in range(NUM_OF_WORDS):\n",
    "                listOfWords[i+k] = bcolors.WARNING + listOfWords[i+k] + bcolors.ENDC\n",
    "#             listOfWords[i+1] = bcolors.WARNING + listOfWords[i+1] + bcolors.ENDC\n",
    "    out_str = ' '.join(listOfWords)\n",
    "    highlight_txt = bcolors.FAIL + 'Highlighted' + bcolors.ENDC\n",
    "    print(highlight_txt, ' - ', out_str)\n",
    "#     print(out_str)\n",
    "\n",
    "def predict_toxicity(toxic_text, word):\n",
    "#     print(toxic_text)\n",
    "    text_pp = glove_preprocess(toxic_text)\n",
    "#     print(text_pp)\n",
    "    tok_text = tokenizer.texts_to_sequences([text_pp])\n",
    "#     print(tok_text)\n",
    "    x = pad_sequences(tok_text, maxlen=MAXLEN)\n",
    "#     print(x.shape)\n",
    "    y = model.predict(x)\n",
    "#     print(word, '-', np.round(y,3))\n",
    "    return y\n",
    "\n",
    "def text_representation(toxic_text):\n",
    "    listOfWords = toxic_text.split()\n",
    "#     print(listOfWords)\n",
    "    nWords = len(listOfWords)\n",
    "    baseline = predict_toxicity(toxic_text, 'BASELINE')\n",
    "    words_to_highlight = []\n",
    "    for i in range(nWords-(NUM_OF_WORDS-1)):\n",
    "        listOfWords = toxic_text.split()\n",
    "        curWord = ' '.join(listOfWords[i:i+2])\n",
    "    #     print('Iter - ', i)\n",
    "    #     print(listOfWords)\n",
    "        for k in range(NUM_OF_WORDS):\n",
    "            del listOfWords[i]\n",
    "    #     del listOfWords[i]\n",
    "        cur_toxic_text = ' '.join(listOfWords)\n",
    "        curValue = predict_toxicity(cur_toxic_text, curWord)\n",
    "        if curValue[0][0] < baseline[0][0]:\n",
    "            words_to_highlight.append(i)\n",
    "#     print(words_to_highlight)        \n",
    "    highlight_sentence(toxic_text, words_to_highlight)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Toxic Comments -  6090\n",
      "\u001b[1mOriginal   \u001b[0m  -  == Arabs are committing genocide in Iraq, but no protests in Europe. == \n",
      "\n",
      " May Europe also burn in hell.\n",
      "\u001b[91mHighlighted\u001b[0m  -  \u001b[93m==\u001b[0m \u001b[93m\u001b[93mArabs\u001b[0m\u001b[0m \u001b[93mare\u001b[0m committing genocide \u001b[93min\u001b[0m \u001b[93mIraq,\u001b[0m \u001b[93mbut\u001b[0m \u001b[93mno\u001b[0m protests in Europe. \u001b[93m==\u001b[0m \u001b[93m\u001b[93mMay\u001b[0m\u001b[0m \u001b[93m\u001b[93mEurope\u001b[0m\u001b[0m \u001b[93m\u001b[93malso\u001b[0m\u001b[0m \u001b[93mburn\u001b[0m \u001b[93min\u001b[0m \u001b[93mhell.\u001b[0m\n",
      "\u001b[1mOriginal   \u001b[0m  -  DJ Robinson is gay as hell! he sucks his dick so much!!!!!\n",
      "\u001b[91mHighlighted\u001b[0m  -  DJ Robinson \u001b[93mis\u001b[0m \u001b[93m\u001b[93mgay\u001b[0m\u001b[0m \u001b[93m\u001b[93mas\u001b[0m\u001b[0m \u001b[93m\u001b[93mhell!\u001b[0m\u001b[0m \u001b[93m\u001b[93mhe\u001b[0m\u001b[0m \u001b[93m\u001b[93msucks\u001b[0m\u001b[0m \u001b[93m\u001b[93mhis\u001b[0m\u001b[0m \u001b[93m\u001b[93mdick\u001b[0m\u001b[0m \u001b[93mso\u001b[0m much!!!!!\n",
      "\u001b[1mOriginal   \u001b[0m  -  :Fuck off, you anti-semitic cunt.  |\n",
      "\u001b[91mHighlighted\u001b[0m  -  \u001b[93m:Fuck\u001b[0m \u001b[93m\u001b[93moff,\u001b[0m\u001b[0m \u001b[93myou\u001b[0m \u001b[93manti-semitic\u001b[0m \u001b[93m\u001b[93mcunt.\u001b[0m\u001b[0m \u001b[93m|\u001b[0m\n",
      "\u001b[1mOriginal   \u001b[0m  -  How dare you vandalize that page about the HMS Beagle! Don't vandalize again, demon!\n",
      "\u001b[91mHighlighted\u001b[0m  -  How \u001b[93mdare\u001b[0m \u001b[93m\u001b[93myou\u001b[0m\u001b[0m \u001b[93mvandalize\u001b[0m that page \u001b[93mabout\u001b[0m \u001b[93m\u001b[93mthe\u001b[0m\u001b[0m \u001b[93m\u001b[93mHMS\u001b[0m\u001b[0m \u001b[93m\u001b[93mBeagle!\u001b[0m\u001b[0m \u001b[93mDon't\u001b[0m \u001b[93mvandalize\u001b[0m \u001b[93m\u001b[93magain,\u001b[0m\u001b[0m \u001b[93mdemon!\u001b[0m\n",
      "\u001b[1mOriginal   \u001b[0m  -  ::No, he is an arrogant, self serving, immature idiot. Get it right.\n",
      "\u001b[91mHighlighted\u001b[0m  -  ::No, he is \u001b[93man\u001b[0m \u001b[93m\u001b[93marrogant,\u001b[0m\u001b[0m \u001b[93mself\u001b[0m serving, \u001b[93mimmature\u001b[0m \u001b[93m\u001b[93midiot.\u001b[0m\u001b[0m \u001b[93mGet\u001b[0m \u001b[93mit\u001b[0m \u001b[93mright.\u001b[0m\n",
      "\u001b[1mOriginal   \u001b[0m  -  :Eek, but shes cute in an earthy kind of way. Cant sing for shit though. Thanks for giving me an unhappy memory.\n",
      "\u001b[91mHighlighted\u001b[0m  -  :Eek, \u001b[93mbut\u001b[0m \u001b[93mshes\u001b[0m cute \u001b[93min\u001b[0m \u001b[93m\u001b[93man\u001b[0m\u001b[0m \u001b[93m\u001b[93mearthy\u001b[0m\u001b[0m \u001b[93m\u001b[93mkind\u001b[0m\u001b[0m \u001b[93mof\u001b[0m \u001b[93mway.\u001b[0m \u001b[93mCant\u001b[0m sing \u001b[93mfor\u001b[0m \u001b[93m\u001b[93mshit\u001b[0m\u001b[0m \u001b[93mthough.\u001b[0m Thanks for \u001b[93mgiving\u001b[0m \u001b[93m\u001b[93mme\u001b[0m\u001b[0m \u001b[93man\u001b[0m \u001b[93munhappy\u001b[0m \u001b[93mmemory.\u001b[0m\n",
      "\u001b[1mOriginal   \u001b[0m  -  ::::Well, it sucks to have a university to be nicknameless. And it's the first time in NCAA history that it has happened. /\n",
      "\u001b[91mHighlighted\u001b[0m  -  ::::Well, \u001b[93mit\u001b[0m \u001b[93m\u001b[93msucks\u001b[0m\u001b[0m \u001b[93mto\u001b[0m \u001b[93mhave\u001b[0m \u001b[93ma\u001b[0m university \u001b[93mto\u001b[0m \u001b[93mbe\u001b[0m \u001b[93mnicknameless.\u001b[0m \u001b[93mAnd\u001b[0m it's \u001b[93mthe\u001b[0m \u001b[93m\u001b[93mfirst\u001b[0m\u001b[0m \u001b[93mtime\u001b[0m in NCAA history that \u001b[93mit\u001b[0m \u001b[93m\u001b[93mhas\u001b[0m\u001b[0m \u001b[93m\u001b[93mhappened.\u001b[0m\u001b[0m \u001b[93m/\u001b[0m\n",
      "\u001b[1mOriginal   \u001b[0m  -  == Argh == \n",
      "\n",
      " Some random idiot deleted the whole Japan article <.\n",
      "\u001b[91mHighlighted\u001b[0m  -  \u001b[93m==\u001b[0m \u001b[93m\u001b[93mArgh\u001b[0m\u001b[0m \u001b[93m==\u001b[0m Some \u001b[93mrandom\u001b[0m \u001b[93m\u001b[93midiot\u001b[0m\u001b[0m \u001b[93mdeleted\u001b[0m the whole Japan article <.\n",
      "\u001b[1mOriginal   \u001b[0m  -  (we hate america and we are going to bomb the shit out of there cities, Queza;s will rain)\n",
      "\u001b[91mHighlighted\u001b[0m  -  (we hate america and \u001b[93mwe\u001b[0m \u001b[93m\u001b[93mare\u001b[0m\u001b[0m \u001b[93m\u001b[93mgoing\u001b[0m\u001b[0m \u001b[93mto\u001b[0m \u001b[93mbomb\u001b[0m \u001b[93m\u001b[93mthe\u001b[0m\u001b[0m \u001b[93m\u001b[93mshit\u001b[0m\u001b[0m \u001b[93mout\u001b[0m of there cities, Queza;s \u001b[93mwill\u001b[0m \u001b[93mrain)\u001b[0m\n",
      "\u001b[1mOriginal   \u001b[0m  -  == Bold textYOU SUCK!!! == \n",
      "\n",
      "  \n",
      "\n",
      " U SUCK HANNAH MONTANA\n",
      "\u001b[91mHighlighted\u001b[0m  -  == Bold \u001b[93mtextYOU\u001b[0m \u001b[93m\u001b[93mSUCK!!!\u001b[0m\u001b[0m \u001b[93m\u001b[93m==\u001b[0m\u001b[0m \u001b[93m\u001b[93mU\u001b[0m\u001b[0m \u001b[93m\u001b[93mSUCK\u001b[0m\u001b[0m \u001b[93mHANNAH\u001b[0m MONTANA\n"
     ]
    }
   ],
   "source": [
    "# Examples\n",
    "idx =test_labels.index[test_labels['toxic'] ==1].tolist()\n",
    "print('Number of Toxic Comments - ', len(idx))\n",
    "for i in range(10):\n",
    "    toxic_idx = idx[i]\n",
    "    toxic_text = test['comment_text'][toxic_idx]\n",
    "    orignial_txt = bcolors.BOLD + 'Original   ' + bcolors.ENDC\n",
    "    print(orignial_txt, ' - ', toxic_text)\n",
    "    text_representation(toxic_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
