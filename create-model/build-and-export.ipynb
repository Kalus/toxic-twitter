{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU,Conv1D,MaxPooling1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D,Bidirectional\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import tensorflowjs as tfjs\n",
    "from keras.models import model_from_json   \n",
    "import unidecode\n",
    "import keras.backend\n",
    "import json\n",
    "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 4} ) \n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 6\n",
    "tfjs_directory = 'ascii-3-model/fit-model-ascii'\n",
    "model_json_file = 'ascii-3-model/model-ascii.json'\n",
    "model_h5_file = \"ascii-3-model/model-ascii.h5\"\n",
    "char_to_index = \"ascii-3-model/ascii-char-map.json\"\n",
    "\n",
    "max_features = 200\n",
    "maxlen = 500\n",
    "embedding_size = 12\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write out tokenizer index\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 500, 12)           864       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 500, 100)          4900      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 125, 100)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 125, 120)          57960     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                6050      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 70,080\n",
      "Trainable params: 70,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('./kaggle/train.csv')\n",
    "test = pd.read_csv('./kaggle/test.csv')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, \n",
    "                                                    train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]], \n",
    "                                                    test_size = 0.10, random_state = 42)\n",
    "\n",
    "list_sentences_train = X_train[\"comment_text\"].apply(unidecode.unidecode)\n",
    "list_sentences_test = X_test[\"comment_text\"].apply(unidecode.unidecode)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features,char_level=True)\n",
    "\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "\n",
    "\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_sentences_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "\n",
    "with open(char_to_index, 'w') as f:\n",
    "    f.write(json.dumps(tokenizer.word_index))\n",
    "    print('write out tokenizer index')\n",
    "\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_sentences_test, maxlen=maxlen)\n",
    "\n",
    "def get_model(embed_size,dropout=0.2):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(len(tokenizer.word_index)+1, embed_size)(inp)\n",
    "    x = Conv1D(filters=100, kernel_size=4, padding='same', activation='relu')(x)\n",
    "    x=MaxPooling1D(pool_size=4)(x)\n",
    "\n",
    "    x = Bidirectional(GRU(60, return_sequences=True,\n",
    "                          name='lstm_layer',dropout=dropout,\n",
    "                          recurrent_dropout=0.2))(x)\n",
    "\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "\n",
    "    x = Dense(50, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                   metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model(embed_size = embedding_size, dropout = dropout)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143613, 500)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/6\n",
      "  5728/143613 [>.............................] - ETA: 16:23 - loss: 0.2021 - acc: 0.9544"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_t,y_train, \n",
    "                 batch_size=batch_size,\n",
    "                 epochs=epochs,\n",
    "                 validation_data=(X_te,y_test)) #callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "tfjs.converters.save_keras_model(model, tfjs_directory)\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(model_json_file, \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(model_h5_file)\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
